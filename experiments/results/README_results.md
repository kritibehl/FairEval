# FairEval â€“ Benchmark Summary

**Source:** `experiments/results/faireval_results.csv`

## Per-model Toxicity Summary (higher = worse)

| model               |     mean |   median |      max |   count |
|:--------------------|---------:|---------:|---------:|--------:|
| sshleifer/tiny-gpt2 | 0.000515 | 0.000354 | 0.000902 |       5 |

> *Composite = toxicity score; lower is better.*