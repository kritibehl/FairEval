# ğŸ§  FairEval â€” Human-Aligned Evaluation for Generative Models

![Tests](https://github.com/kritibehl/FairEval/actions/workflows/tests.yml/badge.svg)
[![codecov](https://codecov.io/gh/kritibehl/FairEval/branch/main/graph/badge.svg)](https://codecov.io/gh/kritibehl/FairEval)

**FairEval** is a lightweight, reproducible framework for evaluating generative models across **safety, fairness, factuality, coherence, and uncertainty** using:
- **Automatic metrics** â€” toxicity, bias slices, EM/F1, ROUGE, BERTScore  
- **LLM-as-Judge** â€” 5-axis rubric (Helpfulness, Faithfulness, Harmlessness, Style, Sensitivity)  
- **Human evaluation** â€” 3 raters/sample, reporting Îº and Ï  
- **Streamlit demo** â€” side-by-side model comparison  

---

## âœ¨ Overview
FairEval unifies **automatic**, **LLM-based**, and **human** evaluations to measure both model quality and ethical alignment.  
It provides an end-to-end pipeline for:
- âš™ï¸ Metric computation and aggregation  
- ğŸ§® Human-AI agreement analysis (Fleissâ€™ Îº, Spearman Ï)  
- ğŸ§© Fairness & toxicity analytics using Detoxify  
- ğŸ“Š Uncertainty estimation for stability checks  
- ğŸ›ï¸ A Streamlit dashboard for model-to-model comparisons  

---

## ğŸ§¾ Abstract
**FairEval: Human-Aligned, Safety-Aware Evaluation for Generative Models**  
We present FairEval, a reproducible evaluation toolkit that combines:
1. **LLM-as-Judge rubric scoring** across helpfulness, faithfulness, harmlessness, style, and sensitivity  
2. **Human reliability analysis** via Fleissâ€™ Îº and Judgeâ†”Human Spearman Ï  
3. **Safety & Fairness analytics** with Detoxify-based toxicity and bias slice reporting  

FairEval supports SQuAD-style EM/F1, BERTScore, and self-consistency uncertainty estimation to quantify model reliability.  
A Streamlit demo enables interactive, side-by-side model evaluation, while a reproducible pipeline ingests human ratings, computes reliability metrics, and visualizes fairness distributions.  
The framework is Python-only, lightweight, and designed for product and research teams seeking to ship safer, more trustworthy model experiences.

---

ğŸ”— Medium Article:
â€œFairEval â€” A Human-Aligned Evaluation Framework for Generative Modelsâ€
https://medium.com/@kriti0608/faireval-a-human-aligned-evaluation-framework-for-generative-models-d822bfd5c99d

## ğŸš€ Quickstart
```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
streamlit run demo/app.py
faireval/
â”œâ”€ config/
â”‚  â”œâ”€ tasks.yaml
â”‚  â””â”€ prompts/judge_rubric.md
â”œâ”€ data/
â”œâ”€ src/
â”‚  â”œâ”€ pipelines/
â”‚  â”œâ”€ models/
â”‚  â”œâ”€ tasks/
â”‚  â”œâ”€ viz/
â”‚  â””â”€ utils/
â”œâ”€ demo/app.py
â”œâ”€ docs/
â””â”€ eval_runs/

